<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Yuchen Liu</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/Cutted_Headshot.png" alt="" /></a>
					<h2><strong>Yuchen Liu</strong></h2>
					
					<p>Research Scientist/Engineer II</p>
					<p>Adobe Research</p>
					<p>yuliu [at] adobe [dot] com</p>
					<!-- how to add link <a href="http://html5up.net">HTML5 UP</a> -->
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="one">
						<header class="major">
							<h2>ABOUT ME</h2>
						</header>

						<p>
						I am a researcher at Adobe Research where I research in visual generative modeling (<a href="pdfs/Yuchen Liu.pdf">CV</a>). Currently, I am interested in few-step generation approachs to address the iterative nature of diffusion models.
						</p>

						<p>
						Before that, I got my Ph.D. in the Department of Electrical and Computer Engineering at Princeton University in Novemeber 2022, fortunately supervised by <a href="https://ee.princeton.edu/people/sun-yuan-kung">Prof. Sun-Yuan Kung</a> and <a href="https://www.princeton.edu/~wentzlaf/">Prof. David Wentzlaff</a>.  
						I received my BEng in Electronic and Computer Engineering from Hong Kong University of Science and Technology (HKUST) in June 2017. 
						</p>

						<p> 
						I spent two wonderful summers at Adobe Research working with <a href="https://zhixinshu.github.io/"> Dr. Zhixin Shu</a>, <a href="https://yijunmaverick.github.io/"> Dr. Yijun Li</a>, <a href="https://research.adobe.com/person/zhe-lin/"> Dr. Zhe Lin</a>, <a href="https://richzhang.github.io/">Dr. Richard Zhang</a>, and <a href="https://fperazzi.github.io/">Dr. Federico Perazzi</a> in 2020 and 2021. 
						<font color=red>
						Adobe Research is open to university collaboration in forms of internship and joint projects for motivated students. If you are interested in doing research projects with me, please drop me an email with your CV.</font>
						</p> 


						
					</section>

				<!-- Two -->
					<section id="two">
						<div class="major">
							<h2>Experience</h2>

							<p><img src="images/Adobe_Logo.png"width="8%"style="float:left;margin-right:1em"><b>Adobe Research, San Jose, CA </b> 
								<br> Research Scientist/Engineer II, July 2024 - Present 
								<br> Research Scientist/Engineer I, December 2022 - July 2024 
								</p>

							<p><img src="images/Adobe_Logo.png"width="8%"style="float:left;margin-right:1em"><b>Adobe Research, San Jose, CA </b> 
								<br> Research Intern, May - November 2021 
								<br>Mentor: Dr. Zhixin Shu</p>


							<p><img src="images/Adobe_Logo.png"width="8%"style="float:left;margin-right:1em"><b>Adobe Research, San Francisco, CA </b> 
								<br> Research Intern, May - November 2020 
								<br>Mentor: Dr. Federico Perazzi and Dr. Zhixin Shu </p>

							<p><img src="images/PrincetonLogo.png"width="8%"style="float:left;margin-right:1em"><b>Princeton University, Princeton, NJ </b> 
								<br> Research Assistant, September 2017 - November 2022 
								<br>Advisor: Prof. Sun-Yuan Kung and Prof. David Wentzlaff </p>


							<p><img src="images/MIT_Logo_2.png"width="8%"style="float:left;margin-right:1em"><b>Massachusetts Institute of Technology, Cambridge, MA</b> 
								<br> Research Intern, June - August 2016
								<br> Advisor: Prof. Dina Katabi </p>

							<!-- <h2>Experience</h2>
							
                                                        <p><img src="images/PrincetonLogo.png"width="8%"style="float:left;margin-right:1em"><b>Princeton University</b>, Princeton, NJ <br> Research Assistant, <a href="http://parallel.princeton.edu">Princeton Parallel Research Group</a><br> Advisor: Prof. David Wentzlaff<br></p>

                                                        <p><img src="images/PrincetonLogo.png"width="8%"style="float:left;margin-right:1em"><b>Princeton University</b>, Princeton, NJ <br> Teaching Assistant, COS/ELE475 Computer Architecture<br><br></p>

							<p><img src="images/UMichLogo.png"width="8%"style="float:left;margin-right:1em"><b>University of Michigan </b>, Ann Arbor, MI (Summer 2016)<br> Research Assistant <br> Advisor: Prof. Trevor Mudge <br></p>

							<p><img src="images/TsinghuaLogo.png"width="8%"style="float:left;margin-right:1em"><b>Tsinghua University </b>, Beijing (9/2016-7/2017) <br> Research Assistant <br> Advisor: Prof. Yangdong Deng <br></p> -->
						

						</div>
					</section>

				<!-- Three -->
					<section id="two">
						<h2>Publication</h2>
							<ul>
							<li><b>X-Fusion: Introducing New Modality to Frozen Large Language Models</b> <br>
                            Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, <u><b>Yuchen Liu</b></u>, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li <br>
							<b>Best Paper</b> of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop of Transformer for Vision (CVPRW 2025). <br>
							(<a href="https://arxiv.org/abs/2504.20996">paper</a>) (<a href="https://sichengmo.github.io/XFusion/">webpage</a>)
							</li>
							<br>							

							<li><b>ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration</b> <br>
                            Yongsheng Yu, Haitian Zheng, Zhifei Zhang, Jianming Zhang, Yuqian Zhou, Connelly Barnes, <u><b>Yuchen Liu</b></u>, Wei Xiong, Zhe Lin, Jiebo Luo <br>
							arXiv preprint arXiv:2504.08591 (2025). <br>
							(<a href="https://arxiv.org/abs/2504.08591">paper</a>)
							</li>
							<br>

							<li><b>Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks</b> <br>
                            Bhishma Dedhia, David Bourgin, Krishna Kumar Singh, Yuheng Li, Yan Kang, Zhan Xu, Niraj K Jha, <u><b>Yuchen Liu</b></u> <br>
							arXiv preprint arXiv:2503.17539 (2025). <br>
							(<a href="https://arxiv.org/abs/2503.17539">paper</a>) (<a href="https://glitchinthematrix.github.io/vins/">webpage</a>)
							</li>
							<br>

							<li><b>DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization</b> <br>
                            Zihan Ding, Chi Jin, Difan Liu, Haitian Zheng, Krishna Kumar Singh, Qiang Zhang, Yan Kang, Zhe Lin, <u><b>Yuchen Liu</b></u> <br>
							arXiv preprint arXiv:2412.15689 (2024). <br>
							(<a href="https://arxiv.org/abs/2412.15689">paper</a>) (<a href="https://quantumiracle.github.io/dollar/">webpage</a>)
							</li>
							<br>
							
							<li><b>Mixture of efficient diffusion experts through automatic interval and sub-network selection</b> <br>
                            Alireza Ganjdanesh, Yan Kang, <u><b>Yuchen Liu</b></u>, Richard Zhang, Zhe Lin, Heng Huang <br>
							European Conference on Computer Vision (ECCV 2024). <br>
							(<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07799.pdf">paper</a>) (<a href="https://arxiv.org/abs/2409.15557">arxiv</a>)
							</li>
							<br>

							<li><b>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</b> <br>
                            Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj Jha, <u><b>Yuchen Liu</b></u><br>
							IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024). <br>
							(<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Attention-Driven_Training-Free_Efficiency_Enhancement_of_Diffusion_Models_CVPR_2024_paper.pdf">paper</a>) (<a href="https://arxiv.org/abs/2405.05252">arxiv</a>) (<a href="https://atedm.github.io">webpage</a>) (<a href="https://www.youtube.com/watch?v=ZmOdN4F_8Ew">video</a>)
							</li>
							<br>

							<li><b>SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model</b> <br>
                            Zhengang Li, Yan Kang, <u><b>Yuchen Liu</b></u>, Difan Liu, Tobias Hinz, Feng Liu, Yanzhi Wang<br>
							IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024). <br>
							(<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SNED_Superposition_Network_Architecture_Search_for_Efficient_Video_Diffusion_Model_CVPR_2024_paper.pdf">paper</a>) (<a href="https://arxiv.org/abs/2406.00195">arxiv</a>)
							</li>
							<br>

							<li><b>Personalized Residuals for Concept-Driven Text-to-Image Generation</b> <br>
                            Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin, <u><b>Yuchen Liu</b></u>, Richard Zhang, Tobias Hinz<br>
							IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024). <br>
							(<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ham_Personalized_Residuals_for_Concept-Driven_Text-to-Image_Generation_CVPR_2024_paper.pdf">paper</a>) (<a href="https://arxiv.org/abs/2405.12978">arxiv</a>)
							</li>
							<br>

							<li><b>Building Efficient Neural Prefetcher</b> <br>
                             <u><b>Yuchen Liu</b></u>, Georgios Tziantzioulis, David Wentzlaff<br>
							International Symposium on Memory Systems (MEMSYS 2023). <br>
							(<a href="https://dl.acm.org/doi/abs/10.1145/3631882.3631903?casa_token=J1x3MHHTxcMAAAAA%3AcEX6ewLK19q9eY-wGGVyvRbkTiA25eeaKnE_5yHR1Z_hKarAqJzuYNCPoKrB3MDWpwcQKRTMrUYRt9Y">paper</a>)
							</li>
							<br>

							<li><b>3D-FM GAN: Towards 3D-Controllable Face Manipulation</b> <br>
                             <u><b>Yuchen Liu</b></u>, Zhixin Shu, Yijun Li, Zhe Lin, Richard Zhang, and S.Y. Kung<br>
							European Conference on Computer Vision (ECCV 2022). <br>
							(<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750106.pdf">paper</a>) (<a href="https://arxiv.org/abs/2208.11257">arxiv</a>) (<a href="https://github.com/adobe/3D-FM-GAN">code</a>) (<a href="https://lychenyoko.github.io/3D-FM-GAN-Webpage/">webpage</a>) (<a href="https://www.youtube.com/watch?v=3tR7qIXyzLE">video</a>) (<a href="https://lychenyoko.github.io/3D-FM-GAN-Webpage/resources/3D-FM%20GAN%20poster.pdf">poster</a>)
							</li>
							<br>
							

							<li><b>Evolving Transferable Neural Pruning Functions</b> <br>
                                 <u><b>Yuchen Liu</b></u>, S.Y. Kung, and David Wentzlaff<br>
							Genetic and Evolutionary Computation Conference (GECCO 2022). <br>
							(<a href="https://dl.acm.org/doi/10.1145/3512290.3528694">paper</a>) (<a href="https://arxiv.org/abs/2110.10876">arxiv</a>) (<a href="https://github.com/lychenyoko/Evolving_Transferable_Pruning_Functions">code</a>) <br>
							</li>
							<br>								

							<li><b>Class-Discriminative CNN Compression</b> <br>
                                 <u><b>Yuchen Liu</b></u>, David Wentzlaff, and S.Y. Kung<br>
							International Conference on Pattern Recognition (ICPR 2022). <br> 
							(<a href="https://arxiv.org/pdf/2110.10864.pdf">paper</a>) (<a href="https://github.com/lychenyoko/Class_Discriminative_Compression">code</a>) <br>
							</li>
							<br>


							<li><b>Content-Aware GAN Compression</b> <br>
                                 <u><b>Yuchen Liu</b></u>, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, and S.Y. Kung<br>
							IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021). <br>
							(<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Content-Aware_GAN_Compression_CVPR_2021_paper.html">paper</a>) (<a href="https://arxiv.org/abs/2104.02244">arxiv</a>) (<a href="https://lychenyoko.github.io/content_aware_gan_compression/">webpage</a>) (<a href="https://github.com/lychenyoko/content-aware-gan-compression">code</a>)<br>
							</li>
							<br>


							<li><b>Rethinking Class-Discrimination Based CNN Channel Pruning</b> <br>
                                 <u><b>Yuchen Liu</b></u>, David Wentzlaff, and S.Y. Kung<br>
							arXiv preprint arXiv:2004.14492 (2020). <br>
							(<a href="https://arxiv.org/pdf/2004.14492.pdf">paper</a>) <br>
							</li>
							<br>

							<li><b>Methodical Design and Trimming of Deep Learning Networks: Enhancing External BP Learning with Internal Omnipresent-supervision Training Paradigm</b> <br>
                            S.Y. Kung, Zejiang Hou, and <u><b>Yuchen Liu</b></u> <br>
							IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019). <br>
							(<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682208">paper</a>) 
							</li>
							<br>

							<li><b>Extracting gait velocity and stride length from surrounding radio signals</b> <br>
                                 Chen-Yu Hsu, <u><b>Yuchen Liu</b></u>, Zachary Kabelac, Rumen Hristov, Dina Katabi, Christine Liu<br>
							ACM Conference on Human Factors in Computing Systems (CHI 2017). <br>
							(<a href="https://dl.acm.org/doi/pdf/10.1145/3025453.3025937">paper</a>) 
							</li>

							</ul>	
					</section>



				<!-- Three Tech Transfers/Industry -->

					<section id="three">
						<h2>Tech Transfers</h2>
							<ul>
							<li><strong>Adobe Firefly Image 2</strong>, core researcher in model efficiency (<a href="https://blog.adobe.com/en/publish/2023/10/10/future-is-firefly-adobe-max">link</a>) 
							<li><strong>Adobe Firefly Image 1</strong> (<a href="https://blog.adobe.com/en/publish/2023/03/21/bringing-gen-ai-to-creative-cloud-adobe-firefly">link</a>)
							</li>
							
							</ul>	
					</section>

				<!-- Four Service -->

					<section id="three">
						<h2>Service</h2>
							<ul>
							<li><strong>Conference Reviewer/Program Committee</strong>  
								<ul style="font-size: 15px;">
									<li> CVPR - 2021, 2023, 2024, 2025 </li>
									<li> NeurIPS - 2021, 2022, 2023, 2024 </li>
									<li> ICML - 2022, 2023, 2024, 2025 </li>
									<li> ICLR - 2024, 2025 </li>
									<li> ECCV/ICCV - 2024, 2025 </li>
									<li> WACV - 2023, 2024 </li>
									<li> AAAI - 2023, 2024 </li>
								</ul>
							<li><strong>Journal Reviewer</strong>  
								<ul style="font-size: 15px;">
									<li> ACM Computing Surveys </li>
								</ul>
							</li>
							
							</ul>	
					</section>

				<!-- Five Interns and Collaborators -->

					<section id="three">
						<h2>Interns and Collaborators</h2>
							<ul>
							<li><a href="https://bhishmadedhia.com">Bhishma Dedhia</a>, Princeton University
							<li><a href="https://quantumiracle.github.io/webpage/">Zihan Ding</a>, Princeton University
							<li><a href="https://harveyp123.github.io">Hongwu Peng</a>, University of Connecticut 
							<li><a href="https://www.linkedin.com/in/dung-dinh-999b2213b/?originalSubdomain=au">Dung Dinh</a>, University of Syndney
							<li><a href="https://sichengmo.github.io">Sicheng Mo</a>, UCLA
							<li><a href="https://www.yongshengyu.com">Yongsheng Yu</a>, Univeristy of Rochester
							<li><a href="https://hongjiew.github.io">Hongjie Wang</a>, Princeton University
							<li><a href="https://alii-ganjj.github.io">Alireza Ganjdanesh</a>, University of Maryland
							<li><a href="https://scholar.google.com/citations?user=hH1Oun0AAAAJ&hl=en">Zhengang Li</a>, Northeastern University
							<li><a href="https://cusuh.github.io">Cusuh Ham</a>, Georgia Institute of Technology
							<li><a href="https://jiaxinxie97.github.io/Jiaxin-Xie/">Jiaxin Xie</a>, Hong Kong University of Science and Technology
							</ul>	
					</section>



				<!-- Six Awards and Honors -->
					<section id="four">
						<h2>Awards and Honors</h2>
							<ul>
							<li><strong>Best Paper Awards in CVPRW</strong>  
								<span style="padding-left: 20px;">May 2025</span> 
								<br> X-Fusion: Introducing New Modality to Frozen Large Language Models.						
							</li>
							<li><strong>HKUST Outstanding Undergraduate</strong>  
								<span style="padding-left: 20px;">May 2017</span>  
								<br> Awareded to top 3% of graduating undergraduate students						
							</li>
							<li><strong>The 14th National Challenge Cup, National Round, Third Prize</strong> 
								<span style="padding-left: 20px;">October 2015</span>    
								<br> Innovation competition joined by more than 2.5 million students from over 3,000 institutions
							</li>
							<li><strong>Scholarship Scheme for Continuing Undergraduate Students</strong>  
								<span style="padding-left: 20px;">2013 - 2017</span>    
								<br> Awarded to the top 5% of undergraduate students
							</li>
							<li><strong>Dean's List</strong>  
								<span style="padding-left: 20px;">2014 - 2017</span>
								<br> Acknowledgement from HKUST's dean to students with excellent academic performance
							</li>

							
							</ul>	
					</section>


			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="https://github.com/lychenyoko" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto:yl16@princeton.edu" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Yuchen Liu</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
